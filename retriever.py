import os
import json
import argparse

# OMP: Error #15: Initializing libomp.dylib... hatasÄ± iÃ§in macOS geÃ§ici Ã§Ã¶zÃ¼mÃ¼
# Bu ayar, OpenMP kullanan herhangi bir kÃ¼tÃ¼phane (Ã¶rn. faiss) import edilmeden Ã¶nce yapÄ±lmalÄ±dÄ±r.
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

from dotenv import load_dotenv
from langchain_core.documents import Document
from langchain_community.vectorstores import FAISS
from langchain_google_genai import GoogleGenerativeAIEmbeddings

# --- CONFIGURATION ---
INDEX_PATH = "faiss_index"
CHUNKS_DIR = "chunks"
EMBEDDING_MODEL = "gemini-embedding-001"

# Desteklenen okul seviyeleri
SUPPORTED_LEVELS = ["anaokulu", "ilkokul", "ortaokul", "lise"]

def initialize_embeddings() -> GoogleGenerativeAIEmbeddings:
    """API anahtarÄ±nÄ± yÃ¼kler ve embedding modelini baÅŸlatÄ±r."""
    load_dotenv()
    google_api_key = os.getenv("GOOGLE_API_KEY")
    if not google_api_key:
        raise ValueError("GOOGLE_API_KEY ortam deÄŸiÅŸkeni bulunamadÄ±. .env dosyasÄ±nÄ± kontrol edin.")
    
    return GoogleGenerativeAIEmbeddings(
        model=EMBEDDING_MODEL,
        google_api_key=google_api_key
    )

def create_embedding_text(item: dict) -> str:
    """
    Chunk'tan embedding iÃ§in zenginleÅŸtirilmiÅŸ metin oluÅŸturur.
    Format: title + question + embedding_hint + content
    """
    parts = []
    
    if item.get("title"):
        parts.append(item["title"])
    
    if item.get("question"):
        parts.append(item["question"])
    
    if item.get("embedding_hint"):
        parts.append(item["embedding_hint"])
    
    if item.get("content"):
        parts.append(item["content"])
    
    return " ".join(parts)

def load_chunks_from_files(levels: list = None) -> list:
    """
    Belirtilen seviyeler iÃ§in chunk'larÄ± yÃ¼kler.
    
    Args:
        levels: YÃ¼klenecek okul seviyeleri listesi. None ise tÃ¼m seviyeler yÃ¼klenir.
    
    Returns:
        TÃ¼m chunk'larÄ±n birleÅŸtirilmiÅŸ listesi.
    """
    if levels is None:
        levels = SUPPORTED_LEVELS
    
    all_chunks = []
    for level in levels:
        json_path = os.path.join(CHUNKS_DIR, f"{level}.json")
        if os.path.exists(json_path):
            with open(json_path, "r", encoding="utf-8") as f:
                chunks = json.load(f)
                all_chunks.extend(chunks)
                print(f"âœ“ {level}.json: {len(chunks)} chunk yÃ¼klendi")
        else:
            print(f"âš  {json_path} bulunamadÄ±, atlanÄ±yor...")
    
    return all_chunks

def create_and_save_index(embedding_model: GoogleGenerativeAIEmbeddings, levels: list = None):
    """JSON'dan dokÃ¼manlarÄ± okur, FAISS indeksini oluÅŸturur ve diske kaydeder."""
    print(f"'{INDEX_PATH}' bulunamadÄ±. Indeks sÄ±fÄ±rdan oluÅŸturuluyor...\n")
    
    # TÃ¼m chunk'larÄ± yÃ¼kle
    raw_data = load_chunks_from_files(levels)
    
    if not raw_data:
        raise ValueError("HiÃ§ chunk yÃ¼klenemedi! chunks/ klasÃ¶rÃ¼nÃ¼ kontrol edin.")

    # Document nesneleri oluÅŸtur - zenginleÅŸtirilmiÅŸ embedding metni ile
    docs = [
        Document(
            page_content=create_embedding_text(item),
            metadata={
                "id": item.get("id"),
                "level": item.get("level"),
                "title": item.get("title"),
                "question": item.get("question"),
                "answer_type": item.get("answer_type"),
                "embedding_hint": item.get("embedding_hint"),
                "source": item.get("source"),
                "tags": item.get("tags"),
                "version": item.get("version"),
                "chunk_index": item.get("chunk_index"),
                "original_content": item.get("content")  # Orijinal iÃ§erik ayrÄ± saklanÄ±yor
            }
        )
        for item in raw_data
    ]
    
    print(f"\n{len(docs)} dokÃ¼man iÃ§in embedding oluÅŸturuluyor ve indeksleniyor. Bu iÅŸlem zaman alabilir...")
    vectorstore = FAISS.from_documents(docs, embedding_model)
    vectorstore.save_local(INDEX_PATH)
    print(f"âœ“ Indeks baÅŸarÄ±yla '{INDEX_PATH}' klasÃ¶rÃ¼ne kaydedildi.")
    return vectorstore

def load_vector_store(embedding_model: GoogleGenerativeAIEmbeddings, levels: list = None, force_recreate: bool = False, silent: bool = False) -> FAISS:
    """
    Mevcut FAISS indeksini diskten yÃ¼kler veya yoksa yenisini oluÅŸturur.
    
    Args:
        embedding_model: KullanÄ±lacak embedding modeli
        levels: YÃ¼klenecek okul seviyeleri (None ise tÃ¼mÃ¼)
        force_recreate: True ise mevcut indeks silinip yeniden oluÅŸturulur
        silent: True ise terminal Ã§Ä±ktÄ±larÄ± bastÄ±rÄ±lÄ±r
    
    Returns:
        FAISS vector store
    """
    if force_recreate and os.path.exists(INDEX_PATH):
        if not silent:
            print(f"âš  Mevcut indeks siliniyor...")
        import shutil
        shutil.rmtree(INDEX_PATH)
    
    if os.path.exists(INDEX_PATH):
        if not silent:
            print(f"âœ“ Mevcut indeks '{INDEX_PATH}' klasÃ¶rÃ¼nden yÃ¼kleniyor...")
        return FAISS.load_local(INDEX_PATH, embedding_model, allow_dangerous_deserialization=True)
    else:
        return create_and_save_index(embedding_model, levels)

def get_retrieved_documents(query: str, k: int = 3, levels: list = None, force_recreate: bool = False, silent: bool = False) -> list:
    """
    Verilen bir sorgu iÃ§in FAISS veritabanÄ±ndan ilgili dokÃ¼manlarÄ± ve skorlarÄ±nÄ± getirir.

    Args:
        query (str): Aranacak metin.
        k (int): DÃ¶ndÃ¼rÃ¼lecek en benzer dokÃ¼man sayÄ±sÄ±.
        levels (list): Filtrelenecek okul seviyeleri (None ise tÃ¼mÃ¼).
        force_recreate (bool): Ä°ndeksi yeniden oluÅŸtur.
        silent (bool): True ise terminal Ã§Ä±ktÄ±larÄ± bastÄ±rÄ±lÄ±r (chatbot kullanÄ±mÄ± iÃ§in).

    Returns:
        list: (Document, score) Ã§iftlerinden oluÅŸan bir liste.
    """
    try:
        embedding_model = initialize_embeddings()
        vectorstore = load_vector_store(embedding_model, levels, force_recreate, silent)
        
        if not silent:
            print(f"\nğŸ” '{query}' sorgusu iÃ§in en benzer {k} sonuÃ§ getiriliyor...")
        
        # TÃ¼m sonuÃ§larÄ± al
        results = vectorstore.similarity_search_with_score(query, k=k*2)  # Daha fazla al, filtrele
        
        # EÄŸer seviye filtresi varsa uygula
        if levels:
            results = [(doc, score) for doc, score in results if doc.metadata.get("level") in levels]
        
        # Ä°stenen sayÄ±da sonuÃ§ dÃ¶ndÃ¼r
        return results[:k]
    except Exception as e:
        if not silent:
            print(f"âŒ Retriever hatasÄ±: {e}")
        return []

def main():
    parser = argparse.ArgumentParser(
        description="FAISS ve LangChain ile Ã§oklu seviye RAG sorgusu yap.",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Ã–rnekler:
  python retriever.py "Anaokulunda Ä°ngilizce dersleri nasÄ±l?"
  python retriever.py "Matematik dersleri" -k 5
  python retriever.py "Ã–dev politikasÄ±" --levels anaokulu ilkokul
  python retriever.py "Vizyon misyon" --recreate
        """
    )
    parser.add_argument("query", type=str, help="VektÃ¶r veritabanÄ±nda aranacak sorgu.")
    parser.add_argument("-k", type=int, default=3, help="DÃ¶ndÃ¼rÃ¼lecek en benzer dokÃ¼man sayÄ±sÄ± (varsayÄ±lan: 3).")
    parser.add_argument("--levels", nargs="+", choices=SUPPORTED_LEVELS, 
                       help="Sadece belirtilen okul seviyelerinde ara (varsayÄ±lan: tÃ¼mÃ¼).")
    parser.add_argument("--recreate", action="store_true", 
                       help="FAISS indeksini yeniden oluÅŸtur (mevcut silinir).")
    args = parser.parse_args()

    try:
        print("=" * 70)
        print("ğŸ“ Ã‡Ã¶zÃ¼m EÄŸitim KurumlarÄ± - RAG Retriever")
        print("=" * 70)
        
        # Seviye filtresi varsa gÃ¶ster
        if args.levels:
            print(f"ğŸ“š Arama kapsamÄ±: {', '.join(args.levels)}")
        else:
            print(f"ğŸ“š Arama kapsamÄ±: TÃ¼m seviyeler ({', '.join(SUPPORTED_LEVELS)})")
        
        # 1. Benzerlik aramasÄ± yap
        results_with_scores = get_retrieved_documents(
            args.query, 
            k=args.k, 
            levels=args.levels,
            force_recreate=args.recreate
        )

        # 2. SonuÃ§larÄ± yazdÄ±r
        if not results_with_scores:
            print("\nâŒ HiÃ§ sonuÃ§ bulunamadÄ±.")
            return

        print(f"\n{'='*70}")
        print(f"ğŸ“Š {len(results_with_scores)} SonuÃ§ Bulundu")
        print(f"{'='*70}")

        for i, (doc, score) in enumerate(results_with_scores, 1):
            print(f"\n{'â”€'*70}")
            print(f"ğŸ”¢ SonuÃ§ {i} | ğŸ“ˆ Benzerlik Skoru: {score:.4f}")
            print(f"{'â”€'*70}")
            print(f"ğŸ·ï¸  ID: {doc.metadata.get('id')}")
            print(f"ğŸ¯ Seviye: {doc.metadata.get('level', 'N/A').upper()}")
            print(f"ğŸ“– BaÅŸlÄ±k: {doc.metadata.get('title')}")
            
            if doc.metadata.get('question'):
                print(f"â“ Soru: {doc.metadata.get('question')}")
            
            if doc.metadata.get('answer_type'):
                print(f"ğŸ’¡ YanÄ±t Tipi: {doc.metadata.get('answer_type')}")
            
            if doc.metadata.get('embedding_hint'):
                print(f"ğŸ”‘ Anahtar Kelimeler: {doc.metadata.get('embedding_hint')}")
            
            # Orijinal iÃ§eriÄŸi gÃ¶ster (varsa)
            content = doc.metadata.get('original_content', doc.page_content)
            print(f"\nğŸ“„ Ä°Ã§erik:")
            print(content[:400] + ("..." if len(content) > 400 else ""))
            
            if doc.metadata.get('tags'):
                tags = doc.metadata.get('tags')
                if isinstance(tags, list):
                    print(f"\nğŸ·ï¸  Etiketler: {', '.join(tags)}")

        print(f"\n{'='*70}")

    except Exception as e:
        print(f"\nâŒ Bir hata oluÅŸtu: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()