"""
NEWS SCRAPER + SEARCH NODE
1. Scraper fonksiyonlarÄ±: Ã‡Ã¶zÃ¼m Koleji web sitesinden haber listesi ve detay Ã§eker
2. LangGraph node: KullanÄ±cÄ± sorgusuyla haberleri arar ve context'e ekler
"""

import requests
from bs4 import BeautifulSoup
import re
import json
import urllib.parse

# ChatState import sadece LangGraph node iÃ§in gerekli
# __main__ test bloÄŸunda kullanÄ±lmaz
try:
    from state_schema import ChatState
except ImportError:
    ChatState = None  # Test modunda gerekmiyor


# ============================================================================
# SCRAPER FUNCTIONS
# ============================================================================

def scrape_news_list(url: str) -> list:
    """
    Ã‡Ã¶zÃ¼m Koleji duyurular liste sayfasÄ±ndan:
    - image
    - title
    - summary
    - detail_url
    - date
    dÃ¶ndÃ¼ren scraper.
    """
    try:
        r = requests.get(url, timeout=10)
        r.raise_for_status()
    except Exception as e:
        return {"error": f"Liste sayfasÄ± alÄ±namadÄ±: {e}"}
    
    soup = BeautifulSoup(r.text, "html.parser")

    items = soup.select("div.col-md-12.mb-4.animated.fadeIn")
    results = []

    for item in items:
        # Ana kart
        card = item.select_one(".card-archive-item")
        if not card:
            continue
        
        # GÃ–RSEL (background-image iÃ§inde)
        img_div = card.select_one(".card__imagery")
        image_url = None
        if img_div and "style" in img_div.attrs:
            match = re.search(r'url\((.*?)\)', img_div["style"])
            if match:
                image_url = match.group(1)

        # BAÅLIK
        title_tag = card.select_one(".card__title a")
        title = title_tag.get_text(strip=True) if title_tag else None
        
        # DETAY LINK
        detail_url = title_tag["href"] if title_tag else None

        # AÃ‡IKLAMA (en uzun olan d-none d-md-block)
        summary_tag = card.select_one(".card__body.d-none.d-md-block")
        summary = summary_tag.get_text(strip=True) if summary_tag else None

        # TARÄ°H
        date_tag = card.select_one(".card__date span.d-none.d-md-block")
        if date_tag:
            date_text = date_tag.get_text(strip=True)
            # "Eklenme Tarihi:" kÄ±smÄ±nÄ± temizle
            date_text = date_text.replace("Eklenme Tarihi:", "").strip()
        else:
            date_text = None
        
        results.append({
            "title": title,
            "summary": summary,
            "image": image_url,
            "detail_url": detail_url,
            "date": date_text
        })
    
    return results


def scrape_news_detail(url: str) -> dict:
    """
    Ã‡Ã¶zÃ¼m Koleji haber detay sayfasÄ±ndan
    - fotoÄŸraf
    - baÅŸlÄ±k
    - iÃ§erik (HTML -> temiz text)
    
    dÃ¶ndÃ¼ren scraper.
    
    NOT: BazÄ± haberlerde tek <p> var (title + content birleÅŸik),
    bazÄ±larÄ±nda ayrÄ±. Her ikisi de handle edilir.
    """
    try:
        r = requests.get(url, timeout=10)
        r.raise_for_status()
    except Exception as e:
        return {"error": f"Sayfa Ã§ekilemedi: {e}"}
    
    soup = BeautifulSoup(r.text, "html.parser")
    
    page_detail = soup.select_one("div.page-detail")
    if not page_detail:
        return {"error": "page-detail div bulunamadÄ±"}

    # FOTO
    img_tag = page_detail.select_one(".news-image img")
    image_url = img_tag["src"] if img_tag and img_tag.has_attr('src') else None

    # BAÅLIK + Ä°Ã‡ERÄ°K
    p_tags = page_detail.select(".not-content p")
    if not p_tags:
        return {"error": "not-content altÄ±nda p etiketi yok"}
    
    # Tek <p> durumu (title + content birlikte)
    if len(p_tags) == 1:
        full_text = p_tags[0].get_text(strip=True)
        # Ä°lk cÃ¼mleyi title olarak al (nokta veya ilk 100 karakter)
        if '.' in full_text[:150]:
            first_sentence_end = full_text.find('.', 0, 150) + 1
            title = full_text[:first_sentence_end].strip()
            content = full_text[first_sentence_end:].strip()
        else:
            # Nokta yoksa ilk 100 karakteri title yap
            title = full_text[:100] + "..." if len(full_text) > 100 else full_text
            content = full_text
    else:
        # Ã‡oklu <p> durumu (klasik: ilk p = title, geri kalanlar = content)
        title = p_tags[0].get_text(strip=True)
        content_paragraphs = [p.get_text(strip=True) for p in p_tags[1:]]
        content = "\n\n".join(content_paragraphs)

    return {
        "title": title,
        "image": image_url,
        "content": content,
    }


# ============================================================================
# LANGGRAPH NODE
# ============================================================================


def news_search_node(state: ChatState) -> ChatState:
    """
    KullanÄ±cÄ±nÄ±n sorusunu title parametresi olarak kullanarak
    Ã‡Ã¶zÃ¼m Koleji duyurular sayfasÄ±ndan haber listesi Ã§eker.
    
    Ä°lk 3 haberin DETAYLARINI da Ã§ekip full content ile birlikte
    LLM'e gÃ¶nderir (daha zengin context).
    
    SonuÃ§larÄ± JSON string olarak state["retrieved_context"] iÃ§ine yazar.
    """
    query = state.get("user_query", "").strip()

    if not query:
        state["retrieved_context"] = "Haber aramasÄ± yapÄ±lamadÄ±: user_query boÅŸ."
        return state

    print("\nğŸ“° [NEWS SEARCH NODE] Haber aramasÄ± baÅŸlÄ±yor...")
    print(f"   ğŸ” Sorgu: {query}")

    # KullanÄ±cÄ± sorusunu URL title parametresi haline getir
    encoded_title = urllib.parse.quote(query)

    # API-like endpoint (bu Ã§alÄ±ÅŸÄ±yor)
    base_url = "https://www.cozumkoleji.com.tr/icerik/duyurular/liste"
    url = f"{base_url}?title={encoded_title}&year="

    print(f"   ğŸŒ Fetching URL: {url}")

    # Scraper'Ä± Ã§alÄ±ÅŸtÄ±r
    data = scrape_news_list(url)

    # Error case
    if isinstance(data, dict) and "error" in data:
        print("   âŒ Scraper hata verdi:", data["error"])
        state["retrieved_context"] = f"Scraper error: {data['error']}"
        return state

    # EÄŸer sonuÃ§ yoksa FALLBACK: Genel sorgu (tÃ¼m haberler)
    if len(data) == 0:
        print("   âš ï¸  Spesifik sonuÃ§ yok, tÃ¼m haberler Ã§ekiliyor...")
        fallback_url = f"{base_url}?title=&year="
        data = scrape_news_list(fallback_url)
        
        # Fallback da baÅŸarÄ±sÄ±z olursa
        if isinstance(data, dict) and "error" in data:
            print("   âŒ Fallback de baÅŸarÄ±sÄ±z:", data["error"])
            state["retrieved_context"] = "Haber listesi alÄ±namadÄ±."
            return state
        
        if len(data) == 0:
            print("   âŒ HiÃ§ haber bulunamadÄ± (fallback).")
            state["retrieved_context"] = "Åu anda gÃ¶rÃ¼ntÃ¼lenebilecek duyuru bulunmuyor."
            return state
        
        print(f"   âœ… Fallback baÅŸarÄ±lÄ±: {len(data)} haber bulundu")

    # Ä°lk 3 haberi alalÄ±m (detay Ã§ekmek iÃ§in)
    top_items = data[:3]

    print(f"   âœ… {len(top_items)} haber bulundu, detaylarÄ± Ã§ekiliyor...")

    # LLM'e gÃ¶nderilecek zengin context
    news_context = []

    for idx, item in enumerate(top_items, 1):
        detail_url = item.get("detail_url")
        
        # Detay sayfasÄ±nÄ± Ã§ek
        if detail_url:
            print(f"   ğŸ“„ {idx}/{len(top_items)}: {item['title'][:50]}...")
            detail_data = scrape_news_detail(detail_url)
            
            if "error" in detail_data:
                # Detay Ã§ekilemezse sadece Ã¶zet kullan
                print(f"      âš ï¸  Detay Ã§ekilemedi, Ã¶zet kullanÄ±lÄ±yor")
                news_context.append({
                    "title": item["title"],
                    "summary": item["summary"],
                    "date": item["date"],
                    "url": detail_url,
                    "image": item["image"],
                    "content": None  # Detay yok
                })
            else:
                # Full content ile birlikte ekle
                content_preview = detail_data["content"][:100] + "..." if len(detail_data["content"]) > 100 else detail_data["content"]
                print(f"      âœ… Detay Ã§ekildi ({len(detail_data['content'])} karakter)")
                
                news_context.append({
                    "title": detail_data["title"],
                    "summary": item["summary"],  # Liste'den gelen Ã¶zet
                    "date": item["date"],
                    "url": detail_url,
                    "image": detail_data["image"],
                    "content": detail_data["content"]  # â† FULL CONTENT!
                })
        else:
            # URL yoksa sadece liste bilgisiyle ekle
            news_context.append({
                "title": item["title"],
                "summary": item["summary"],
                "date": item["date"],
                "url": None,
                "image": item["image"],
                "content": None
            })

    print(f"   ğŸ¯ {len(news_context)} haber detayÄ± LLM'e gÃ¶nderiliyor")

    # State'e ekle (stringify)
    state["retrieved_context"] = json.dumps(news_context, ensure_ascii=False, indent=2)

    return state

# ============================================================================
# TEST
# ============================================================================

if __name__ == "__main__":
    print("="*80)
    print("TEST 1: DUYURU LÄ°STESÄ°")
    print("="*80)
    test_list_url = "https://www.cozumkoleji.com.tr/icerik/duyurular/liste?title=Ã§Ã¶zÃ¼m&year="
    data_list = scrape_news_list(test_list_url)
    
    if isinstance(data_list, dict) and "error" in data_list:
        print(f"âŒ HATA: {data_list['error']}")
    else:
        print(f"âœ… {len(data_list)} duyuru bulundu\n")
        for i, item in enumerate(data_list[:3], 1):
            print(f"{i}. {item['title']}")
            print(f"   ğŸ“… {item['date']}")
            print(f"   ğŸ”— {item['detail_url']}")
            print(f"   ğŸ“ {item['summary'][:80] if item['summary'] else '(Ã¶zet yok)'}...")
            print()
    
    print("\n" + "="*80)
    print("TEST 2: DUYURU DETAYI")
    print("="*80)
    test_detail_url = "https://www.cozumkoleji.com.tr/tr/duyuru/744/lgs-yks-hazirlik-kampimiz-tum-hiziyla-basladi"
    data_detail = scrape_news_detail(test_detail_url)
    
    if "error" in data_detail:
        print(f"âŒ HATA: {data_detail['error']}")
    else:
        print(f"âœ… BaÅŸlÄ±k: {data_detail['title']}")
        print(f"ğŸ–¼ï¸  GÃ¶rsel: {data_detail['image']}")
        print(f"ğŸ“„ Ä°Ã§erik ({len(data_detail['content'])} karakter):")
        print(data_detail['content'][:200] + "...")

    print("\n" + "="*80)
    print("TEST 3: NEWS_SEARCH_NODE (FULL WORKFLOW)")
    print("="*80)
    
    # Mock state (genel sorgu - boÅŸ title ile tÃ¼m haberler)
    mock_state = {
        "user_query": "Ã§Ã¶zÃ¼m",  # Ã‡alÄ±ÅŸan bir sorgu (TEST 1'deki gibi)
        "retrieved_context": ""
    }
    
    result_state = news_search_node(mock_state)
    
    print("\nğŸ“‹ LLM'E GÃ–NDERÄ°LEN CONTEXT:")
    print("="*80)
    
    context_data = json.loads(result_state["retrieved_context"])
    
    for idx, item in enumerate(context_data, 1):
        print(f"\n{idx}. {item['title']}")
        print(f"   ğŸ“… {item['date']}")
        print(f"   ğŸ”— {item.get('url', 'URL yok')}")
        
        if item.get("content"):
            content_len = len(item["content"])
            preview = item["content"][:150] + "..." if content_len > 150 else item["content"]
            print(f"   âœ… Full Content ({content_len} karakter):")
            print(f"      {preview}")
        else:
            print(f"   âš ï¸  Sadece Ã¶zet:")
            print(f"      {item['summary'][:100]}...")
    
    print("\n" + "="*80)
    print("âœ… TÃœM TESTLER TAMAMLANDI")
    print("="*80)
