import os
from typing import Annotated, TypedDict
from dotenv import load_dotenv
from operator import add

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage

from langgraph.graph import StateGraph, END, START
from langgraph.checkpoint.memory import MemorySaver

from retriever import get_retrieved_documents, SUPPORTED_LEVELS

# --- CONFIGURATION ---
CHAT_MODEL = "gemini-2.5-flash"

# --- STATE SCHEMA (TypedDict for LangGraph) ---
class ChatState(TypedDict):
    """State for the chat graph."""
    levels: list[str] | None
    messages: Annotated[list[BaseMessage], add]  # add operator appends messages
    context: str

def initialize_chat_model() -> ChatGoogleGenerativeAI:
    """API anahtarÄ±nÄ± yÃ¼kler ve sohbet modelini baÅŸlatÄ±r."""
    load_dotenv()
    google_api_key = os.getenv("GOOGLE_API_KEY")
    if not google_api_key:
        raise ValueError("GOOGLE_API_KEY ortam deÄŸiÅŸkeni bulunamadÄ±. .env dosyasÄ±nÄ± kontrol edin.")
    
    return ChatGoogleGenerativeAI(
        model=CHAT_MODEL,
        google_api_key=google_api_key,
        temperature=0.4,  # TutarlÄ± ama doÄŸal yanÄ±tlar iÃ§in (0.0=deterministik, 1.0=yaratÄ±cÄ±)
    )

def format_context(documents: list) -> str:
    """AlÄ±nan dokÃ¼manlarÄ± LLM'e verilecek tek bir metin bloÄŸu haline getirir."""
    if not documents:
        return "Bilgi bulunamadÄ±."
    
    context = []
    for i, (doc, score) in enumerate(documents, 1):
        level = doc.metadata.get('level', 'N/A').upper()
        title = doc.metadata.get('title', 'BaÅŸlÄ±k yok')
        content = doc.metadata.get('original_content', doc.page_content)
        context.append(f"[{level}] {title}\n{content}")
    return "\n\n---\n\n".join(context)

def get_level_display_name(level: str) -> str:
    """Seviye kodunu kullanÄ±cÄ± dostu isme Ã§evirir."""
    mapping = {
        "anaokulu": "Anaokulu",
        "ilkokul": "Ä°lkokul (1-4. SÄ±nÄ±f)",
        "ortaokul": "Ortaokul (5-8. SÄ±nÄ±f)",
        "lise": "Lise (9-12. SÄ±nÄ±f)"
    }
    return mapping.get(level, level)

def welcome_and_get_levels() -> list:
    """Veliyi karÅŸÄ±lar ve Ã§ocuklarÄ±nÄ±n eÄŸitim kademelerini Ã¶ÄŸrenir."""
    print("\n" + "="*70)
    print("ğŸ“ Ã‡Ã¶zÃ¼m EÄŸitim KurumlarÄ± - AI Veli AsistanÄ±")
    print("="*70)
    print("\nMerhaba! Ben Ã‡Ã¶zÃ¼m Koleji AI asistanÄ±yÄ±m.")
    print("Size okulumuz hakkÄ±nda bilgi vermek iÃ§in buradayÄ±m. ğŸ˜Š\n")
    
    # Seviye seÃ§imi
    print("LÃ¼tfen Ã§ocuÄŸunuzun/Ã§ocuklarÄ±nÄ±zÄ±n eÄŸitim kademesini seÃ§in:")
    print("(Birden fazla Ã§ocuÄŸunuz varsa, virgÃ¼lle ayÄ±rarak girebilirsiniz)\n")
    
    for i, level in enumerate(SUPPORTED_LEVELS, 1):
        print(f"{i}. {get_level_display_name(level)}")
    print(f"{len(SUPPORTED_LEVELS) + 1}. TÃ¼m kademeler")
    
    while True:
        choice = input("\nSeÃ§iminiz (Ã¶rn: 1,2 veya 1): ").strip()
        
        if not choice:
            print("âŒ LÃ¼tfen bir seÃ§im yapÄ±n.")
            continue
        
        # "TÃ¼m kademeler" seÃ§eneÄŸi
        if choice == str(len(SUPPORTED_LEVELS) + 1):
            return SUPPORTED_LEVELS
        
        # Birden fazla seÃ§im
        try:
            choices = [int(c.strip()) for c in choice.split(',')]
            selected_levels = []
            
            for c in choices:
                if 1 <= c <= len(SUPPORTED_LEVELS):
                    selected_levels.append(SUPPORTED_LEVELS[c - 1])
                else:
                    print(f"âŒ GeÃ§ersiz seÃ§im: {c}")
                    break
            else:
                if selected_levels:
                    print("\nâœ… SeÃ§ilen kademeler:")
                    for level in selected_levels:
                        print(f"   â€¢ {get_level_display_name(level)}")
                    print("\nArtÄ±k bu kademelere Ã¶zel sorular sorabilirsiniz!")
                    return selected_levels
        except ValueError:
            print("âŒ LÃ¼tfen geÃ§erli sayÄ±lar girin (Ã¶rn: 1,2 veya 3)")

def show_help():
    """YardÄ±m mesajÄ±nÄ± gÃ¶sterir."""
    print("\n" + "â”€"*70)
    print("ğŸ“ Komutlar:")
    print("â”€"*70)
    print("  /help veya /yardim    - Bu yardÄ±m mesajÄ±nÄ± gÃ¶sterir")
    print("  /seviye veya /kademe  - EÄŸitim kademesini deÄŸiÅŸtirir")
    print("  /temizle veya /clear  - Sohbet geÃ§miÅŸini temizler")
    print("  /cikis veya /exit     - Programdan Ã§Ä±kar")
    print("â”€"*70)

# --- GRAPH NODES ---
def classify_query(user_msg: str) -> str:
    """Classify user query to decide if retrieval is needed."""
    user_msg_lower = user_msg.lower().strip()
    
    # Greetings - no retrieval needed
    greetings = ["merhaba", "selam", "gÃ¼naydÄ±n", "iyi gÃ¼nler", "hey", "hi"]
    if any(g in user_msg_lower for g in greetings) and len(user_msg_lower) < 20:
        return "greeting"
    
    # Thanks - no retrieval needed
    thanks = ["teÅŸekkÃ¼r", "saÄŸol", "eyv", "tÅŸk"]
    if any(t in user_msg_lower for t in thanks):
        return "thanks"
    
    # Identity questions - no retrieval needed
    identity = ["sen kimsin", "kim olduÄŸun", "adÄ±n ne", "ne yapÄ±yorsun", "ne iÅŸe yarar"]
    if any(i in user_msg_lower for i in identity):
        return "identity"
    
    # Goodbye - no retrieval needed
    goodbye = ["gÃ¶rÃ¼ÅŸÃ¼rÃ¼z", "hoÅŸÃ§akal", "bay", "gÃ¼le gÃ¼le"]
    if any(g in user_msg_lower for g in goodbye):
        return "goodbye"
    
    # Everything else needs retrieval (school-related questions)
    return "question"

def router_node(state: ChatState) -> dict:
    """Route to retrieval or direct LLM based on query type."""
    if not state.get("messages"):
        return {"context": ""}
    
    # Get last user message and check conversation history
    last_user_msg = None
    has_recent_context = False
    
    messages = state.get("messages", [])
    for i, msg in enumerate(reversed(messages)):
        if isinstance(msg, HumanMessage) and last_user_msg is None:
            last_user_msg = msg.content
        # Check if there's a recent AI response (within last 2 messages)
        if isinstance(msg, AIMessage) and i < 2:
            # If AI just responded, user might be asking follow-up
            has_recent_context = True
            break
    
    if not last_user_msg:
        return {"context": ""}
    
    # Classify query
    query_type = classify_query(last_user_msg)
    
    # If not a question, skip retrieval
    if query_type != "question":
        return {"context": ""}  # Empty context, LLM will use general knowledge
    
    # Check if it's a short follow-up question (likely referring to previous context)
    is_short_followup = len(last_user_msg.strip()) < 30 and has_recent_context
    
    if is_short_followup:
        # Keep existing context, don't retrieve again
        return {}  # Don't change context, LLM will use conversation history
    
    # For new questions, mark that retrieval is needed
    return {"context": "NEEDS_RETRIEVAL"}

def retrieve_node(state: ChatState) -> dict:
    """Retrieve relevant documents based on last user message."""
    # Check if retrieval is needed (router marks it)
    if state.get("context") != "NEEDS_RETRIEVAL":
        return {}  # Skip retrieval, keep existing context
    
    if not state.get("messages"):
        return {"context": ""}
    
    # Get last user message
    last_user_msg = None
    for msg in reversed(state["messages"]):
        if isinstance(msg, HumanMessage):
            last_user_msg = msg.content
            break
    
    if not last_user_msg or not state.get("levels"):
        return {"context": ""}
    
    # Retrieve documents
    retrieved_docs = get_retrieved_documents(
        last_user_msg,
        k=4,
        levels=state.get("levels", []),
        force_recreate=False,
        silent=True
    )
    
    # Format context
    if not retrieved_docs:
        context = "Bilgi bulunamadÄ±."
    else:
        context_parts = []
        for i, (doc, score) in enumerate(retrieved_docs, 1):
            level = doc.metadata.get('level', 'N/A').upper()
            title = doc.metadata.get('title', 'BaÅŸlÄ±k yok')
            content = doc.metadata.get('original_content', doc.page_content)
            context_parts.append(f"[{level}] {title}\n{content}")
        context = "\n\n---\n\n".join(context_parts)
    
    # Return context to be used by LLM (store in state for next node)
    # Don't add to messages, just pass it through state
    return {"context": context}

def llm_node(state: ChatState, llm: ChatGoogleGenerativeAI) -> dict:
    """Generate response using LLM with context and full conversation history."""
    if not state.get("messages"):
        return {}
    
    # Get context from state (set by retrieve_node)
    context = state.get("context", "")
    
    # Build system prompt
    level_info = ", ".join([get_level_display_name(l) for l in state.get("levels", [])]) if state.get("levels") else "HenÃ¼z seÃ§ilmedi"
    
    system_prompt = f"""Sen Ã‡Ã¶zÃ¼m EÄŸitim KurumlarÄ±'nÄ±n veli asistanÄ±sÄ±n. SeÃ§ili kademeler: {level_info}

KURALLAR:
1. SADECE BAÄLAM'daki bilgileri kullan, asla uydurma
2. Profesyonel ama samimi ol ("siz" diye hitap et)
3. YanÄ±t uzunluÄŸu soruya gÃ¶re deÄŸiÅŸebilir:
   - Basit sorular (merhaba, teÅŸekkÃ¼r): 1 cÃ¼mle
   - Genel sorular (okul hakkÄ±nda): 2-3 cÃ¼mle + liste
   - DetaylÄ± sorular (program, eÄŸitim): TÃ¼m ilgili bilgiyi ver, BAÄLAM'dan kopyala
4. Gereksiz tekrar yapma, Ã¶zlÃ¼ ol ama eksik bÄ±rakma
5. TAKIP SORULARI: EÄŸer Ã¶nceki yanÄ±tÄ±nla ilgili soru sorulursa (Ã¶rn: "kaÃ§ saat?", "peki ÅŸu?"):
   - Ã–nceki konuÅŸma geÃ§miÅŸini kullan
   - Sadece sorulan spesifik bilgiyi ver

Ã–RNEKLER:

Veli: "Ä°ngilizce eÄŸitimi nasÄ±l?"
Asistan: "Ä°lkokulda Ä°ngilizce eÄŸitimi Cambridge programÄ± ile haftada 12 saat Main Course ve 2 saat Think&Talk dersi ÅŸeklinde verilmektedir. Dil DuÅŸu yÃ¶ntemi ile erken yaÅŸta dil edinimi desteklenmektedir."

Veli: "Okul hakkÄ±nda bilgi istiyorum"
Asistan: "Okulumuz modern bir eÄŸitim anlayÄ±ÅŸÄ± ile Ã¶ÄŸrenci geliÅŸimine odaklanmaktadÄ±r. Size hangi konuda detaylÄ± bilgi verebilirim? â€¢ EÄŸitim programlarÄ± â€¢ Ä°ngilizce eÄŸitimi â€¢ Sosyal aktiviteler â€¢ Ãœcretler â€¢ Servis hizmetleri"

Veli: "Merhaba"
Asistan: "Merhaba! Ben Ã‡Ã¶zÃ¼m EÄŸitim KurumlarÄ±'nÄ±n veli asistanÄ±yÄ±m. Size nasÄ±l yardÄ±mcÄ± olabilirim?"

Veli: "TeÅŸekkÃ¼r ederim"
Asistan: "Rica ederim! BaÅŸka sorunuz olursa Ã§ekinmeyin."

KADEME DEÄÄ°ÅÄ°KLÄ°ÄÄ°:
- FarklÄ± kademe sorulursa: "Åu an {level_info} iÃ§in bilgi verebiliyorum. [Kademe] hakkÄ±nda da bilgi almak ister misiniz?"
- EVET denirse: "Harika! [Kademe] bilgilerini ekledim. #KADEME_EKLE:[kademe]#"

---
BAÄLAM: {context if context else "Genel sohbet, okula Ã¶zgÃ¼ bilgi gerekmiyor."}"""
    
    # Pass FULL conversation history + system prompt
    # This allows LLM to see previous messages for follow-up questions
    messages = [
        SystemMessage(content=system_prompt),
        *state["messages"]  # Include ALL previous messages (HumanMessage and AIMessage)
    ]
    
    # Invoke LLM with complete conversation history
    response = llm.invoke(messages)
    
    return {"messages": [AIMessage(content=response.content)]}

class ChatSession:
    """LangGraph-based chat session manager."""
    
    def __init__(self, llm: ChatGoogleGenerativeAI, checkpointer: MemorySaver):
        self.llm = llm
        self.checkpointer = checkpointer
        self.graph = self._build_graph()
        self.thread_id = "default"  # Can be changed for multi-user scenarios
    
    def _build_graph(self) -> StateGraph:
        """Build the LangGraph workflow."""
        # Create graph
        workflow = StateGraph(ChatState)
        
        # Add nodes
        workflow.add_node("router", router_node)  # New: classify query first
        workflow.add_node("retrieve", retrieve_node)
        workflow.add_node("llm", lambda state: llm_node(state, self.llm))
        
        # Add edges - router first, then conditional retrieval
        workflow.add_edge(START, "router")
        workflow.add_edge("router", "retrieve")
        workflow.add_edge("retrieve", "llm")
        workflow.add_edge("llm", END)
        
        # Compile with checkpointer
        return workflow.compile(checkpointer=self.checkpointer)
    
    def get_config(self):
        """Get configuration with thread_id."""
        return {"configurable": {"thread_id": self.thread_id}}
    
    def get_state(self) -> dict:
        """Get current state as dict."""
        state = self.graph.get_state(self.get_config())
        if state and state.values:
            # Ensure all keys have defaults
            values = state.values
            return {
                "levels": values.get("levels"),
                "messages": values.get("messages", []),
                "context": values.get("context", "")
            }
        return {"levels": None, "messages": [], "context": ""}
    
    def clear_history(self):
        """Clear conversation history by resetting thread."""
        self.thread_id = f"thread_{os.urandom(8).hex()}"
        print("\nâœ… Sohbet geÃ§miÅŸi temizlendi.")
    
    def set_levels(self, levels: list[str]):
        """Set education levels in state."""
        # Simply update state with new levels
        self.graph.update_state(self.get_config(), {"levels": levels})
        print(f"\nâœ… Kademeler gÃ¼ncellendi: {', '.join([get_level_display_name(l) for l in levels])}")
    
    def chat(self, user_query: str) -> str:
        """Send user message and get response."""
        try:
            # Simply invoke with the new message - let the graph handle state
            # The 'add' operator will automatically append to existing messages
            result = self.graph.invoke(
                {"messages": [HumanMessage(content=user_query)]},
                self.get_config()
            )
            
            # Extract last AI message from result
            if result and "messages" in result:
                for msg in reversed(result["messages"]):
                    if isinstance(msg, AIMessage):
                        response = msg.content
                        
                        # Check for level change tags
                        import re
                        tag_pattern = r'#KADEME_EKLE:(\w+)#'
                        matches = re.findall(tag_pattern, response)
                        
                        if matches:
                            # Extract and add new levels
                            current_state = self.get_state()
                            current_levels = current_state.get("levels", [])
                            
                            for level_to_add in matches:
                                level_to_add = level_to_add.lower()
                                if level_to_add in SUPPORTED_LEVELS and level_to_add not in current_levels:
                                    current_levels.append(level_to_add)
                            
                            # Update state with new levels
                            if current_levels != current_state.get("levels", []):
                                self.graph.update_state(self.get_config(), {"levels": current_levels})
                            
                            # Remove tags from response
                            response = re.sub(tag_pattern, '', response).strip()
                        
                        return response
            
            return "ÃœzgÃ¼nÃ¼m, bir yanÄ±t Ã¼retemedim. LÃ¼tfen sorunuzu farklÄ± ÅŸekilde sormayÄ± deneyin."
        except Exception as e:
            print(f"\nâš ï¸ Hata: {e}")
            import traceback
            traceback.print_exc()
            return f"Bir hata oluÅŸtu: {str(e)}"

def main():
    """Main CLI loop with LangGraph-based chat."""
    try:
        # 1. Initialize LLM and checkpointer
        llm = initialize_chat_model()
        checkpointer = MemorySaver()
        
        # 2. Create session
        session = ChatSession(llm, checkpointer)
        
        # 3. Welcome and select levels
        selected_levels = welcome_and_get_levels()
        session.set_levels(selected_levels)
        
        # 4. Show help
        print("\n" + "="*70)
        print("ğŸ’¬ Sohbet baÅŸladÄ±! ArtÄ±k sorularÄ±nÄ±zÄ± sorabilirsiniz.")
        show_help()
        print("="*70)
        
        # 5. Main chat loop
        while True:
            try:
                user_input = input("\nğŸ‘¤ Siz: ").strip()
                
                if not user_input:
                    continue
                
                user_input_lower = user_input.lower()
                
                if user_input_lower in ["/exit", "/cikis", "exit", "quit"]:
                    print("\nğŸ‘‹ GÃ¶rÃ¼ÅŸmek Ã¼zere! Ä°yi gÃ¼nler dileriz.")
                    break
                
                elif user_input_lower in ["/help", "/yardim"]:
                    show_help()
                    continue
                
                elif user_input_lower in ["/seviye", "/kademe"]:
                    new_levels = welcome_and_get_levels()
                    session.set_levels(new_levels)
                    continue
                
                elif user_input_lower in ["/temizle", "/clear"]:
                    session.clear_history()
                    continue
                
                # Normal chat
                print("\nğŸ¤– Asistan: ", end="", flush=True)
                response = session.chat(user_input)
                print(response)
                
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ Program sonlandÄ±rÄ±ldÄ±. Ä°yi gÃ¼nler!")
                break
            except Exception as inner_e:
                print(f"\nâš ï¸ Bir hata oluÅŸtu: {inner_e}")
                print("LÃ¼tfen tekrar deneyin veya /help yazarak yardÄ±m alÄ±n.")

    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ Program sonlandÄ±rÄ±ldÄ±.")
    except Exception as e:
        print(f"\nâŒ Beklenmedik bir hata oluÅŸtu: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
