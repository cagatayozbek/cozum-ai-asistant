import os
from typing import Annotated, TypedDict
from dotenv import load_dotenv
from operator import add

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage

from langgraph.graph import StateGraph, END, START
from langgraph.checkpoint.memory import MemorySaver

from retriever import get_retrieved_documents, SUPPORTED_LEVELS

# --- CONFIGURATION ---
CHAT_MODEL = "gemini-2.5-flash"

# --- STATE SCHEMA (TypedDict for LangGraph) ---
class ChatState(TypedDict):
    """State for the chat graph."""
    levels: list[str] | None
    messages: Annotated[list[BaseMessage], add]  # add operator appends messages
    context: str

def initialize_chat_model() -> ChatGoogleGenerativeAI:
    """API anahtarÄ±nÄ± yÃ¼kler ve sohbet modelini baÅŸlatÄ±r."""
    load_dotenv()
    google_api_key = os.getenv("GOOGLE_API_KEY")
    if not google_api_key:
        raise ValueError("GOOGLE_API_KEY ortam deÄŸiÅŸkeni bulunamadÄ±. .env dosyasÄ±nÄ± kontrol edin.")
    
    return ChatGoogleGenerativeAI(
        model=CHAT_MODEL,
        google_api_key=google_api_key,
        temperature=0.4,  # TutarlÄ± ama doÄŸal yanÄ±tlar iÃ§in (0.0=deterministik, 1.0=yaratÄ±cÄ±)
    )

def format_context(documents: list) -> str:
    """AlÄ±nan dokÃ¼manlarÄ± LLM'e verilecek tek bir metin bloÄŸu haline getirir."""
    if not documents:
        return "Bilgi bulunamadÄ±."
    
    context = []
    for i, (doc, score) in enumerate(documents, 1):
        level = doc.metadata.get('level', 'N/A').upper()
        title = doc.metadata.get('title', 'BaÅŸlÄ±k yok')
        content = doc.metadata.get('original_content', doc.page_content)
        context.append(f"[{level}] {title}\n{content}")
    return "\n\n---\n\n".join(context)

def get_level_display_name(level: str) -> str:
    """Seviye kodunu kullanÄ±cÄ± dostu isme Ã§evirir."""
    mapping = {
        "anaokulu": "Anaokulu",
        "ilkokul": "Ä°lkokul (1-4. SÄ±nÄ±f)",
        "ortaokul": "Ortaokul (5-8. SÄ±nÄ±f)",
        "lise": "Lise (9-12. SÄ±nÄ±f)"
    }
    return mapping.get(level, level)

def welcome_and_get_levels() -> list:
    """Veliyi karÅŸÄ±lar ve Ã§ocuklarÄ±nÄ±n eÄŸitim kademelerini Ã¶ÄŸrenir."""
    print("\n" + "="*70)
    print("ğŸ“ Ã‡Ã¶zÃ¼m EÄŸitim KurumlarÄ± - AI Veli AsistanÄ±")
    print("="*70)
    print("\nMerhaba! Ben Ã‡Ã¶zÃ¼m Koleji AI asistanÄ±yÄ±m.")
    print("Size okulumuz hakkÄ±nda bilgi vermek iÃ§in buradayÄ±m. ğŸ˜Š\n")
    
    # Seviye seÃ§imi
    print("LÃ¼tfen Ã§ocuÄŸunuzun/Ã§ocuklarÄ±nÄ±zÄ±n eÄŸitim kademesini seÃ§in:")
    print("(Birden fazla Ã§ocuÄŸunuz varsa, virgÃ¼lle ayÄ±rarak girebilirsiniz)\n")
    
    for i, level in enumerate(SUPPORTED_LEVELS, 1):
        print(f"{i}. {get_level_display_name(level)}")
    print(f"{len(SUPPORTED_LEVELS) + 1}. TÃ¼m kademeler")
    
    while True:
        choice = input("\nSeÃ§iminiz (Ã¶rn: 1,2 veya 1): ").strip()
        
        if not choice:
            print("âŒ LÃ¼tfen bir seÃ§im yapÄ±n.")
            continue
        
        # "TÃ¼m kademeler" seÃ§eneÄŸi
        if choice == str(len(SUPPORTED_LEVELS) + 1):
            return SUPPORTED_LEVELS
        
        # Birden fazla seÃ§im
        try:
            choices = [int(c.strip()) for c in choice.split(',')]
            selected_levels = []
            
            for c in choices:
                if 1 <= c <= len(SUPPORTED_LEVELS):
                    selected_levels.append(SUPPORTED_LEVELS[c - 1])
                else:
                    print(f"âŒ GeÃ§ersiz seÃ§im: {c}")
                    break
            else:
                if selected_levels:
                    print("\nâœ… SeÃ§ilen kademeler:")
                    for level in selected_levels:
                        print(f"   â€¢ {get_level_display_name(level)}")
                    print("\nArtÄ±k bu kademelere Ã¶zel sorular sorabilirsiniz!")
                    return selected_levels
        except ValueError:
            print("âŒ LÃ¼tfen geÃ§erli sayÄ±lar girin (Ã¶rn: 1,2 veya 3)")

def show_help():
    """YardÄ±m mesajÄ±nÄ± gÃ¶sterir."""
    print("\n" + "â”€"*70)
    print("ğŸ“ Komutlar:")
    print("â”€"*70)
    print("  /help veya /yardim    - Bu yardÄ±m mesajÄ±nÄ± gÃ¶sterir")
    print("  /seviye veya /kademe  - EÄŸitim kademesini deÄŸiÅŸtirir")
    print("  /temizle veya /clear  - Sohbet geÃ§miÅŸini temizler")
    print("  /cikis veya /exit     - Programdan Ã§Ä±kar")
    print("â”€"*70)

# --- GRAPH NODES ---
def classify_query_with_llm(llm: ChatGoogleGenerativeAI, user_msg: str, conversation_history: str = "") -> str:
    """LLM ile soru tipini sÄ±nÄ±flandÄ±r."""
    
    classification_prompt = f"""Sen bir soru sÄ±nÄ±flandÄ±rÄ±cÄ±sÄ±n. KullanÄ±cÄ±nÄ±n mesajÄ±nÄ± analiz et ve TEK KELÄ°ME ile yanÄ±t ver.

SINIFLAR:
- greeting: SelamlaÅŸma (merhaba, selam, gÃ¼naydÄ±n)
- thanks: TeÅŸekkÃ¼r (teÅŸekkÃ¼r ederim, saÄŸol)
- identity: Kimlik sorusu (sen kimsin, ne yapÄ±yorsun)
- goodbye: Veda (hoÅŸÃ§akal, gÃ¶rÃ¼ÅŸÃ¼rÃ¼z)
- followup: Ã–nceki yanÄ±tla ilgili takip sorusu (kaÃ§ saat, ne zaman, hangi gÃ¼n - Ã§ok kÄ±sa)
- question: Okul hakkÄ±nda YENÄ° soru (retrieval gerekli)

Ã–RNEKLER:
KullanÄ±cÄ±: "Merhaba" â†’ greeting
KullanÄ±cÄ±: "TeÅŸekkÃ¼rler" â†’ thanks
KullanÄ±cÄ±: "KaÃ§ saat?" (Ã¶nceki: "Matematik dersi var") â†’ followup
KullanÄ±cÄ±: "manevi eÄŸitim var mÄ±" â†’ question
KullanÄ±cÄ±: "yarÄ±ÅŸmalara katÄ±lÄ±yor musunuz" â†’ question

SON KONUÅMA: {conversation_history if conversation_history else "Yok"}
KULLANICI: "{user_msg}"

SINIF:"""
    
    response = llm.invoke([HumanMessage(content=classification_prompt)])
    classification = response.content.strip().lower()
    
    # Fallback: EÄŸer LLM beklenmeyen bir ÅŸey dÃ¶ndÃ¼rÃ¼rse
    valid_classes = ["greeting", "thanks", "identity", "goodbye", "followup", "question"]
    if classification not in valid_classes:
        classification = "question"  # GÃ¼venli taraf: retrieval yap
    
    print(f"ğŸ¤– DEBUG - LLM Classification: '{user_msg}' â†’ {classification}")
    return classification

def router_node(state: ChatState, llm: ChatGoogleGenerativeAI) -> dict:
    """Route to retrieval or direct LLM based on query type - LLM ile akÄ±llÄ± sÄ±nÄ±flandÄ±rma."""
    if not state.get("messages"):
        return {"context": ""}
    
    # Get last user message and conversation context
    last_user_msg = None
    conversation_context = ""
    
    messages = state.get("messages", [])
    for i, msg in enumerate(reversed(messages)):
        if isinstance(msg, HumanMessage) and last_user_msg is None:
            last_user_msg = msg.content
        # Get last 2 messages for context
        if i < 4:  # Son 4 mesaj (2 soru-cevap Ã§ifti)
            if isinstance(msg, AIMessage):
                conversation_context = f"AI: {msg.content[:100]}... " + conversation_context
            elif isinstance(msg, HumanMessage) and msg.content != last_user_msg:
                conversation_context = f"User: {msg.content} " + conversation_context
    
    if not last_user_msg:
        return {"context": ""}
    
    # LLM ile sÄ±nÄ±flandÄ±r
    query_type = classify_query_with_llm(llm, last_user_msg, conversation_context)
    
    # Retrieval gerekmeyenler
    if query_type in ["greeting", "thanks", "identity", "goodbye"]:
        return {"context": ""}  # Empty context, LLM will use general knowledge
    
    # Takip sorusu - mevcut context'i koru
    if query_type == "followup":
        print(f"ğŸ”„ DEBUG - Follow-up detected by LLM, skipping retrieval: '{last_user_msg}'")
        return {}  # Don't change context, LLM will use conversation history
    
    # Yeni soru - retrieval gerekli
    return {"context": "NEEDS_RETRIEVAL"}

def retrieve_node(state: ChatState) -> dict:
    """Retrieve relevant documents based on last user message."""
    # Check if retrieval is needed (router marks it)
    if state.get("context") != "NEEDS_RETRIEVAL":
        return {}  # Skip retrieval, keep existing context
    
    if not state.get("messages"):
        return {"context": ""}
    
    # Get last user message
    last_user_msg = None
    for msg in reversed(state["messages"]):
        if isinstance(msg, HumanMessage):
            last_user_msg = msg.content
            break
    
    if not last_user_msg or not state.get("levels"):
        return {"context": ""}
    
    # Retrieve documents
    retrieved_docs = get_retrieved_documents(
        last_user_msg,
        k=4,
        levels=state.get("levels", []),
        force_recreate=False,
        silent=True
    )
    
    # DEBUG: Print retrieval results
    print(f"\nğŸ” DEBUG - Retrieved {len(retrieved_docs)} docs for query: '{last_user_msg}'")
    print(f"ğŸ“Š DEBUG - Levels filter: {state.get('levels', [])}")
    if retrieved_docs:
        for i, (doc, score) in enumerate(retrieved_docs[:2], 1):  # Ä°lk 2 sonuÃ§
            print(f"   #{i} [{doc.metadata.get('level')}] Score: {score:.3f} - {doc.metadata.get('title', 'N/A')}")
    
    # Format context
    if not retrieved_docs:
        context = "Bilgi bulunamadÄ±."
        print("âš ï¸  DEBUG - No documents found!")
    else:
        context_parts = []
        for i, (doc, score) in enumerate(retrieved_docs, 1):
            level = doc.metadata.get('level', 'N/A').upper()
            title = doc.metadata.get('title', 'BaÅŸlÄ±k yok')
            content = doc.metadata.get('original_content', doc.page_content)
            context_parts.append(f"[{level}] {title}\n{content}")
        context = "\n\n---\n\n".join(context_parts)
        print(f"âœ… DEBUG - Context created ({len(context)} chars)")
    
    # Return context to be used by LLM (store in state for next node)
    # Don't add to messages, just pass it through state
    return {"context": context}

def llm_node(state: ChatState, llm: ChatGoogleGenerativeAI) -> dict:
    """Generate response using LLM with context and full conversation history."""
    if not state.get("messages"):
        return {}
    
    # Get context from state (set by retrieve_node)
    context = state.get("context", "")
    
    # Build system prompt
    level_info = ", ".join([get_level_display_name(l) for l in state.get("levels", [])]) if state.get("levels") else "HenÃ¼z seÃ§ilmedi"
    
    system_prompt = f"""Sen, Ã‡Ã¶zÃ¼m EÄŸitim KurumlarÄ±'nÄ±n veli asistanÄ±sÄ±n. SeÃ§ili kademeler: {level_info}

KURALLAR:
1. SADECE BAÄLAM'daki bilgileri kullan, asla uydurma
2. Profesyonel ama samimi ol ("siz" diye hitap et)
3. YanÄ±t uzunluÄŸu soruya gÃ¶re deÄŸiÅŸebilir:
   - Basit sorular (merhaba, teÅŸekkÃ¼r): 1 cÃ¼mle
   - Genel sorular (okul hakkÄ±nda): 2-3 cÃ¼mle + liste
   - DetaylÄ± sorular (program, eÄŸitim): TÃ¼m ilgili bilgiyi ver, BAÄLAM'dan kopyala
4. Gereksiz tekrar yapma, Ã¶zlÃ¼ ol ama eksik bÄ±rakma
5. TAKIP SORULARI: EÄŸer Ã¶nceki yanÄ±tÄ±nla ilgili soru sorulursa (Ã¶rn: "kaÃ§ saat?", "peki ÅŸu?"):
   - Ã–nceki konuÅŸma geÃ§miÅŸini kullan
   - Sadece sorulan spesifik bilgiyi ver

Ã–RNEKLER:

Veli: "Ä°ngilizce eÄŸitimi nasÄ±l?"
Asistan: "Ä°lkokulda Ä°ngilizce eÄŸitimi Cambridge programÄ± ile haftada 12 saat Main Course ve 2 saat Think&Talk dersi ÅŸeklinde verilmektedir. Dil DuÅŸu yÃ¶ntemi ile erken yaÅŸta dil edinimi desteklenmektedir."

Veli: "Okul hakkÄ±nda bilgi istiyorum"
Asistan: "Okulumuz modern bir eÄŸitim anlayÄ±ÅŸÄ± ile Ã¶ÄŸrenci geliÅŸimine odaklanmaktadÄ±r. Size hangi konuda detaylÄ± bilgi verebilirim? â€¢ EÄŸitim programlarÄ± â€¢ Ä°ngilizce eÄŸitimi â€¢ Sosyal aktiviteler â€¢ Ãœcretler â€¢ Servis hizmetleri"

Veli: "Merhaba"
Asistan: "Merhaba! Ben Ã‡Ã¶zÃ¼m EÄŸitim KurumlarÄ±'nÄ±n veli asistanÄ±yÄ±m. Size nasÄ±l yardÄ±mcÄ± olabilirim?"

Veli: "TeÅŸekkÃ¼r ederim"
Asistan: "Rica ederim! BaÅŸka sorunuz olursa Ã§ekinmeyin."

KADEME DEÄÄ°ÅÄ°KLÄ°ÄÄ°:
- FarklÄ± kademe sorulursa: "Åu an {level_info} iÃ§in bilgi verebiliyorum. [Kademe] hakkÄ±nda da bilgi almak ister misiniz?"
- EVET denirse: "Harika! [Kademe] bilgilerini ekledim. #KADEME_EKLE:[kademe]#"

---
BAÄLAM: {context if context else "Genel sohbet, okula Ã¶zgÃ¼ bilgi gerekmiyor."}"""
    
    # Pass FULL conversation history + system prompt
    # This allows LLM to see previous messages for follow-up questions
    messages = [
        SystemMessage(content=system_prompt),
        *state["messages"]  # Include ALL previous messages (HumanMessage and AIMessage)
    ]
    
    # Invoke LLM with complete conversation history
    response = llm.invoke(messages)
    
    return {"messages": [AIMessage(content=response.content)]}

class ChatSession:
    """LangGraph-based chat session manager."""
    
    def __init__(self, llm: ChatGoogleGenerativeAI, checkpointer: MemorySaver):
        self.llm = llm
        self.checkpointer = checkpointer
        self.graph = self._build_graph()
        self.thread_id = "default"  # Can be changed for multi-user scenarios
    
    def _build_graph(self) -> StateGraph:
        """Build the LangGraph workflow."""
        # Create graph
        workflow = StateGraph(ChatState)
        
        # Add nodes - pass llm to router for classification
        workflow.add_node("router", lambda state: router_node(state, self.llm))
        workflow.add_node("retrieve", retrieve_node)
        workflow.add_node("llm", lambda state: llm_node(state, self.llm))
        
        # Add edges - router first, then conditional retrieval
        workflow.add_edge(START, "router")
        workflow.add_edge("router", "retrieve")
        workflow.add_edge("retrieve", "llm")
        workflow.add_edge("llm", END)
        
        # Compile with checkpointer
        return workflow.compile(checkpointer=self.checkpointer)
    
    def get_config(self):
        """Get configuration with thread_id."""
        return {"configurable": {"thread_id": self.thread_id}}
    
    def get_state(self) -> dict:
        """Get current state as dict."""
        state = self.graph.get_state(self.get_config())
        if state and state.values:
            # Ensure all keys have defaults
            values = state.values
            return {
                "levels": values.get("levels"),
                "messages": values.get("messages", []),
                "context": values.get("context", "")
            }
        return {"levels": None, "messages": [], "context": ""}
    
    def clear_history(self):
        """Clear conversation history by resetting thread."""
        self.thread_id = f"thread_{os.urandom(8).hex()}"
        print("\nâœ… Sohbet geÃ§miÅŸi temizlendi.")
    
    def set_levels(self, levels: list[str]):
        """Set education levels in state."""
        # Simply update state with new levels
        self.graph.update_state(self.get_config(), {"levels": levels})
        print(f"\nâœ… Kademeler gÃ¼ncellendi: {', '.join([get_level_display_name(l) for l in levels])}")
    
    def chat(self, user_query: str) -> str:
        """Send user message and get response."""
        try:
            # Simply invoke with the new message - let the graph handle state
            # The 'add' operator will automatically append to existing messages
            result = self.graph.invoke(
                {"messages": [HumanMessage(content=user_query)]},
                self.get_config()
            )
            
            # Extract last AI message from result
            if result and "messages" in result:
                for msg in reversed(result["messages"]):
                    if isinstance(msg, AIMessage):
                        response = msg.content
                        
                        # Check for level change tags
                        import re
                        tag_pattern = r'#KADEME_EKLE:(\w+)#'
                        matches = re.findall(tag_pattern, response)
                        
                        if matches:
                            # Extract and add new levels
                            current_state = self.get_state()
                            current_levels = current_state.get("levels", [])
                            
                            for level_to_add in matches:
                                level_to_add = level_to_add.lower()
                                if level_to_add in SUPPORTED_LEVELS and level_to_add not in current_levels:
                                    current_levels.append(level_to_add)
                            
                            # Update state with new levels
                            if current_levels != current_state.get("levels", []):
                                self.graph.update_state(self.get_config(), {"levels": current_levels})
                            
                            # Remove tags from response
                            response = re.sub(tag_pattern, '', response).strip()
                        
                        return response
            
            return "ÃœzgÃ¼nÃ¼m, bir yanÄ±t Ã¼retemedim. LÃ¼tfen sorunuzu farklÄ± ÅŸekilde sormayÄ± deneyin."
        except Exception as e:
            print(f"\nâš ï¸ Hata: {e}")
            import traceback
            traceback.print_exc()
            return f"Bir hata oluÅŸtu: {str(e)}"

def main():
    """Main CLI loop with LangGraph-based chat."""
    try:
        # 1. Initialize LLM and checkpointer
        llm = initialize_chat_model()
        checkpointer = MemorySaver()
        
        # 2. Create session
        session = ChatSession(llm, checkpointer)
        
        # 3. Welcome and select levels
        selected_levels = welcome_and_get_levels()
        session.set_levels(selected_levels)
        
        # 4. Show help
        print("\n" + "="*70)
        print("ğŸ’¬ Sohbet baÅŸladÄ±! ArtÄ±k sorularÄ±nÄ±zÄ± sorabilirsiniz.")
        show_help()
        print("="*70)
        
        # 5. Main chat loop
        while True:
            try:
                user_input = input("\nğŸ‘¤ Siz: ").strip()
                
                if not user_input:
                    continue
                
                user_input_lower = user_input.lower()
                
                if user_input_lower in ["/exit", "/cikis", "exit", "quit"]:
                    print("\nğŸ‘‹ GÃ¶rÃ¼ÅŸmek Ã¼zere! Ä°yi gÃ¼nler dileriz.")
                    break
                
                elif user_input_lower in ["/help", "/yardim"]:
                    show_help()
                    continue
                
                elif user_input_lower in ["/seviye", "/kademe"]:
                    new_levels = welcome_and_get_levels()
                    session.set_levels(new_levels)
                    continue
                
                elif user_input_lower in ["/temizle", "/clear"]:
                    session.clear_history()
                    continue
                
                # Normal chat
                print("\nğŸ¤– Asistan: ", end="", flush=True)
                response = session.chat(user_input)
                print(response)
                
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ Program sonlandÄ±rÄ±ldÄ±. Ä°yi gÃ¼nler!")
                break
            except Exception as inner_e:
                print(f"\nâš ï¸ Bir hata oluÅŸtu: {inner_e}")
                print("LÃ¼tfen tekrar deneyin veya /help yazarak yardÄ±m alÄ±n.")

    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ Program sonlandÄ±rÄ±ldÄ±.")
    except Exception as e:
        print(f"\nâŒ Beklenmedik bir hata oluÅŸtu: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
