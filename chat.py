import os
from typing import Annotated, TypedDict, Literal
from enum import Enum
from dotenv import load_dotenv
from operator import add

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage
from langchain_core.documents import Document
from pydantic import BaseModel, ConfigDict, Field

from langgraph.graph import StateGraph, END, START
from langgraph.checkpoint.memory import MemorySaver

from retriever import get_retrieved_documents, SUPPORTED_LEVELS

import traceback

# --- CONFIGURATION ---
CHAT_MODEL = "gemini-2.5-flash"

# --- ENUMS ---
class RetrievalStatus(str, Enum):
    """Status for document retrieval."""
    NOT_NEEDED = "not_needed"
    PENDING = "pending"
    COMPLETED = "completed"

# --- STATE SCHEMA (TypedDict for LangGraph) ---
class ChatState(TypedDict):
    """State for the chat graph."""
    levels: list[str] | None
    messages: Annotated[list[BaseMessage], add]  # add operator appends messages
    retrieved_docs: list[tuple[Document, float]]  # RAG dokÃ¼manlarÄ± (formatlanmamÄ±ÅŸ)
    retrieval_status: RetrievalStatus

def initialize_chat_model() -> ChatGoogleGenerativeAI:
    """API anahtarÄ±nÄ± yÃ¼kler ve sohbet modelini baÅŸlatÄ±r."""
    load_dotenv()
    google_api_key = os.getenv("GOOGLE_API_KEY")
    if not google_api_key:
        raise ValueError("GOOGLE_API_KEY ortam deÄŸiÅŸkeni bulunamadÄ±. .env dosyasÄ±nÄ± kontrol edin.")
    
    return ChatGoogleGenerativeAI(
        model=CHAT_MODEL,
        google_api_key=google_api_key,
        temperature=0.4,  # TutarlÄ± ama doÄŸal yanÄ±tlar iÃ§in (0.0=deterministik, 1.0=yaratÄ±cÄ±)
    )

def get_level_display_name(level: str) -> str:
    """Seviye kodunu kullanÄ±cÄ± dostu isme Ã§evirir."""
    mapping = {
        "anaokulu": "Anaokulu",
        "ilkokul": "Ä°lkokul (1-4. SÄ±nÄ±f)",
        "ortaokul": "Ortaokul (5-8. SÄ±nÄ±f)",
        "lise": "Lise (9-12. SÄ±nÄ±f)"
    }
    return mapping.get(level, level)

# --- TYPE DEFINITIONS ---
QueryType = Literal["casual", "followup", "question"]


class QueryClassification(BaseModel):
    """Structured response schema for query classification."""
    model_config = ConfigDict(extra="forbid")

    category: QueryType


class LevelDetection(BaseModel):
    """Structured response for detecting education level mentions in user query."""
    model_config = ConfigDict(extra="forbid")

    detected_levels: list[str] = Field(
        default_factory=list,
        description="List of education levels mentioned (anaokulu, ilkokul, ortaokul, lise)"
    )
    should_add_to_context: bool = Field(
        default=False,
        description="Whether these levels should be added to user's context"
    )


# --- HELPER FUNCTIONS ---
def get_last_user_message(messages: list[BaseMessage]) -> str | None:
    """Sohbet geÃ§miÅŸinden son kullanÄ±cÄ± mesajÄ±nÄ± Ã§Ä±kar."""
    for msg in reversed(messages):
        if isinstance(msg, HumanMessage):
            return msg.content
    return None


def format_retrieved_context(retrieved_docs: list[tuple[Document, float]]) -> str:
    """AlÄ±nan dokÃ¼manlarÄ± LLM iÃ§in baÄŸlam string'ine dÃ¶nÃ¼ÅŸtÃ¼r."""
    if not retrieved_docs:
        return "Bilgi bulunamadÄ±."
    
    context_parts = []
    for doc, score in retrieved_docs:
        level = doc.metadata.get('level', 'N/A').upper()
        title = doc.metadata.get('title', 'BaÅŸlÄ±k yok')
        content = doc.metadata.get('original_content', doc.page_content)
        context_parts.append(f"[{level}] {title}\n{content}")
    
    return "\n\n---\n\n".join(context_parts)


def detect_level_mentions(llm: ChatGoogleGenerativeAI, user_query: str) -> LevelDetection:
    """KullanÄ±cÄ± sorgusunda kademe bahsini tespit et - yapÄ±landÄ±rÄ±lmÄ±ÅŸ LLM Ã§Ä±ktÄ±sÄ± kullanarak."""
    detection_prompt = f"""KullanÄ±cÄ±nÄ±n bu sorgusunda hangi eÄŸitim kademelerinden bahsettiÄŸini analiz et ve bunlarÄ±n baÄŸlama eklenip eklenmeyeceÄŸine karar ver.

KullanÄ±cÄ± Sorgusu: "{user_query}"

EÄŸitim Kademeleri:
- anaokulu 
- ilkokul (1-4. sÄ±nÄ±f)
- ortaokul (5-8. sÄ±nÄ±f)
- lise (9-12. sÄ±nÄ±f)

Kurallar:
1. Sadece kullanÄ±cÄ± aÃ§Ä±kÃ§a bir kademe hakkÄ±nda bilgi istiyorsa tespit et
2. YENÄ° bir kademe soruyorlarsa should_add_to_context=true yap
3. Sadece karÅŸÄ±laÅŸtÄ±rma yapÄ±yorlarsa veya zaten o kademeyi konuÅŸuyorsanÄ±z ekleme

Ã–rnekler:
"Lise programÄ± nasÄ±l?" â†’ detected_levels: ["lise"], should_add_to_context: true
"Ä°ngilizce kaÃ§ saat?" â†’ detected_levels: [], should_add_to_context: false
"Lise ve ortaokul karÅŸÄ±laÅŸtÄ±r" â†’ detected_levels: ["lise", "ortaokul"], should_add_to_context: true

Tespit edilen kademeleri JSON nesnesi olarak dÃ¶ndÃ¼r."""
    
    structured_detector = llm.with_structured_output(LevelDetection, method="json_schema")
    
    try:
        result = structured_detector.invoke([HumanMessage(content=detection_prompt)])
        if isinstance(result, LevelDetection):
            return result
        elif isinstance(result, dict):
            return LevelDetection(**result)
        else:
            return LevelDetection()
    except Exception:
        # Fallback to empty detection on error
        return LevelDetection()

# --- GRAPH NODES ---
def classify_query_with_llm(llm: ChatGoogleGenerativeAI, user_msg: str, recent_messages: list[BaseMessage]) -> QueryType:
    """LLM ile soru tipini sÄ±nÄ±flandÄ±r - 3 kategori: casual, followup, question."""
    
    # Son 3 mesajdan baÄŸlam oluÅŸtur (mevcut mesaj hariÃ§)
    # TAM MESAJI AL - Kesme yok! Uzun AI yanÄ±tlarÄ± iÃ§in kritik
    context_parts = []
    for msg in recent_messages[-3:]:
        msg_type = "KullanÄ±cÄ±" if isinstance(msg, HumanMessage) else "Asistan"
        context_parts.append(f"{msg_type}: {msg.content}")
    
    conversation_history = "\n".join(context_parts) if context_parts else "Ä°lk mesaj - takip sorusu olamaz"
    
    classification_prompt = f"""Bu mesajÄ± TEK bir kategoriye sÄ±nÄ±flandÄ±r:
- casual: SelamlaÅŸma, teÅŸekkÃ¼r, veda, kimlik sorusu (merhaba, teÅŸekkÃ¼rler, hoÅŸÃ§akal, sen kimsin)
- followup: Ã–nceki yanÄ±ta baÄŸÄ±mlÄ± takip sorusu (tek baÅŸÄ±na anlamsÄ±z)
- question: Okul hakkÄ±nda baÄŸÄ±msÄ±z yeni soru (veri tabanÄ± aramasÄ± gerekli)

Ã–NCEKÄ° BAÄLAM:
{conversation_history}

ÅU ANKÄ° MESAJ: "{user_msg}"

ANA TEST: "Bu mesaj Ã¶nceki yanÄ±t olmadan anlamlÄ± mÄ±?"
â†’ SelamlaÅŸma/teÅŸekkÃ¼r/veda = casual
â†’ KÄ±sa/belirsiz + Ã¶nceki yanÄ±ta baÄŸlÄ± = followup
â†’ Spesifik okul sorusu = question

Ã–RNEKLER:
"Merhaba" / "TeÅŸekkÃ¼rler" / "Sen kimsin?" â†’ casual
"KaÃ§ saat?" (program tartÄ±ÅŸmasÄ±ndan sonra) â†’ followup
"Ãœcret ne kadar?" (hizmet tartÄ±ÅŸmasÄ±ndan sonra) â†’ followup
"manevi eÄŸitim var mÄ±" â†’ question
"Ä°ngilizce eÄŸitimi nasÄ±l?" â†’ question

SADECE kategori adÄ±nÄ± dÃ¶ndÃ¼r:"""
    
    structured_classifier = llm.with_structured_output(QueryClassification, method="json_schema")
    response = structured_classifier.invoke([HumanMessage(content=classification_prompt)])
    if isinstance(response, QueryClassification):
        classification = response.category
    elif isinstance(response, dict):
        classification = str(response.get("category", ""))
    else:
        classification = str(response)
    classification = classification.strip().lower()
    
    # Validate and fallback
    valid_classes: list[QueryType] = ["casual", "followup", "question"]
    if classification not in valid_classes:
        classification = "question"  # GÃ¼venli taraf: retrieval yap
    
    return classification

def router_node(state: ChatState, llm: ChatGoogleGenerativeAI) -> dict:
    """Soru sÄ±nÄ±flandÄ±rmasÄ±na gÃ¶re retrieval'a veya direkt LLM'e yÃ¶nlendir."""
    messages = state.get("messages", [])
    
    if not messages:
        return {
            "retrieval_status": RetrievalStatus.NOT_NEEDED
        }
    
    # YardÄ±mcÄ± fonksiyon ile son kullanÄ±cÄ± mesajÄ±nÄ± al
    last_user_msg = get_last_user_message(messages)
    if not last_user_msg:
        return {
            "retrieval_status": RetrievalStatus.NOT_NEEDED
        }
    
    # SÄ±nÄ±flandÄ±rÄ±cÄ±ya son mesaj geÃ§miÅŸini gÃ¶nder (mevcut mesaj hariÃ§)
    recent_messages = messages[:-1] if len(messages) > 1 else []
    query_type = classify_query_with_llm(llm, last_user_msg, recent_messages)
    
    # SÄ±radan sohbet veya takip sorusu - retrieval atla
    if query_type in ["casual", "followup"]:
        return {
            "retrieval_status": RetrievalStatus.NOT_NEEDED
        }
    
    # Yeni soru - retrieval gerekli
    return {
        "retrieval_status": RetrievalStatus.PENDING
    }


def decide_next_node(state: ChatState) -> Literal["retrieve", "llm"]:
    """Retrieval durumuna gÃ¶re sonraki node'u belirle."""
    if state.get("retrieval_status") == RetrievalStatus.PENDING:
        return "retrieve"
    return "llm"

def retrieve_node(state: ChatState) -> dict:
    """Son kullanÄ±cÄ± mesajÄ±na gÃ¶re ilgili dokÃ¼manlarÄ± getir ve state'e kaydet."""
    messages = state.get("messages", [])
    if not messages:
        return {
            "retrieved_docs": [],
            "retrieval_status": RetrievalStatus.COMPLETED
        }
    
    # YardÄ±mcÄ± fonksiyon ile son kullanÄ±cÄ± mesajÄ±nÄ± al
    last_user_msg = get_last_user_message(messages)
    if not last_user_msg or not state.get("levels"):
        return {
            "retrieved_docs": [],
            "retrieval_status": RetrievalStatus.COMPLETED
        }
    
    # DokÃ¼manlarÄ± getir (formatlamadan kaydet - LLM node'da formatlanacak)
    retrieved_docs = get_retrieved_documents(
        last_user_msg,
        k=4,
        levels=state.get("levels", []),
        force_recreate=False,
        silent=True
    )
    
    # numpy.float32 â†’ Python float (MemorySaver serialization iÃ§in)
    serializable_docs = [
        (doc, float(score)) for doc, score in retrieved_docs
    ]
    
    return {
        "retrieved_docs": serializable_docs,
        "retrieval_status": RetrievalStatus.COMPLETED
    }

def llm_node(state: ChatState, llm: ChatGoogleGenerativeAI) -> dict:
    """LLM'i Ã§aÄŸÄ±r - sistem promptu dinamik oluÅŸtur, messages state'inden gelir."""
    if not state.get("messages"):
        return {}
    
    # State'ten dokÃ¼manlarÄ± al (varsa)
    retrieved_docs = state.get("retrieved_docs", [])
    
    # DokÃ¼manlarÄ± LLM iÃ§in formatla (sadece burada, state'te tutma)
    if retrieved_docs:
        context = format_retrieved_context(retrieved_docs)
    else:
        context = ""
    
    # Sistem promptunu oluÅŸtur - her Ã§aÄŸrÄ±da dinamik (ama messages'a ekleme!)
    level_info = ", ".join([get_level_display_name(l) for l in state.get("levels", [])]) if state.get("levels") else "HenÃ¼z seÃ§ilmedi"
    
    system_prompt = f"""Siz Ã‡Ã¶zÃ¼m EÄŸitim KurumlarÄ±'nÄ±n veli asistanÄ±sÄ±nÄ±z.
SeÃ§ili kademeler: {level_info}

KILAVUZ:
1) YalnÄ±zca BAÄLAM'daki bilgileri kullanÄ±n; asla uydurma yapmayÄ±n.
2) Resmi fakat samimi bir Ã¼slupla "siz" diye hitap edin.
3) YanÄ±ta kÄ±sa bir Ã¶zetle baÅŸlayÄ±n; ardÄ±ndan Ã§oÄŸu durumda ayrÄ±ntÄ±lÄ± ve kapsamlÄ± aÃ§Ä±klama verin â€” gerekirse birkaÃ§ paragraf, maddeleme ve Ã¶rneklerle destekleyin. Sadece selamlaÅŸma/teÅŸekkÃ¼r gibi durumlarda Ã§ok kÄ±sa olun.
4) Takip sorularÄ±nda Ã¶nceki konuÅŸma geÃ§miÅŸini kullanÄ±n ve sadece sorulan spesifik bilgiyi verin; yine de gerekiyorsa baÄŸlamÄ± geniÅŸletecek ek aÃ§Ä±klamalar ekleyin.
5) BAÄLAM'da ilgili bilgi yoksa: "ÃœzgÃ¼nÃ¼m, bu konuda size yardÄ±mcÄ± olamÄ±yorum." deyin ve veliyi okula yÃ¶nlendirin.
6) Ãœcretlerle ilgili soru geldiÄŸinde fiyat vermeyin; "Ãœcret bilgisi iÃ§in lÃ¼tfen okulla iletiÅŸime geÃ§in." ÅŸeklinde yÃ¶nlendirin.
7) Gereksiz tekrar ve dolgu cÃ¼mlelerinden kaÃ§Ä±nÄ±n; ancak bilgi aktarÄ±mÄ± iÃ§in gerekli aÃ§Ä±klamalarÄ± atlamayÄ±n.

Ã–RNEKLER:
Veli: "Ä°ngilizce eÄŸitimi nasÄ±l?"
Asistan: "Ä°lkokul (1-4): Cambridge programÄ± â€” HaftalÄ±k: 12 saat Main Course, 2 saat Think&Talk. Dil DuÅŸu yÃ¶ntemiyle erken yaÅŸta desteklenir.
Detaylar:
â€¢ Ders yapÄ±sÄ±: ... 
â€¢ DeÄŸerlendirme: ...
â€¢ Ã–neriler: ...

Veli: "Okul hakkÄ±nda bilgi istiyorum"
Asistan: "Okulumuz modern bir eÄŸitim anlayÄ±ÅŸÄ±yla Ã¶ÄŸrenci geliÅŸimine odaklanÄ±r. Hangi konuda detay istersiniz? â€¢ EÄŸitim programlarÄ± â€¢ Ä°ngilizce â€¢ Sosyal aktiviteler â€¢ Ãœcretler â€¢ Servis"

Veli: "Merhaba"
Asistan: "Merhaba! Ben Ã‡Ã¶zÃ¼m EÄŸitim KurumlarÄ±'nÄ±n veli asistanÄ±yÄ±m. Size nasÄ±l yardÄ±mcÄ± olabilirim?"

BAÄLAM: {context if context else "Genel sohbet; okula Ã¶zgÃ¼ bilgi gerekmiyor."}"""
    
    # Ã–NEMLI: SystemMessage sadece invoke'a gÃ¶nder, state'e ekleme!
    # MemorySaver sadece HumanMessage ve AIMessage'larÄ± saklamalÄ±
    messages_for_llm = [
        SystemMessage(content=system_prompt),
        *state["messages"]  # State'ten gelen conversation history
    ]
    
    # LLM'i Ã§aÄŸÄ±r
    response = llm.invoke(messages_for_llm)
    
    # SADECE AI yanÄ±tÄ±nÄ± state'e ekle (SystemMessage DEÄÄ°L!)
    return {"messages": [AIMessage(content=response.content)]}

class ChatSession:
    """LangGraph tabanlÄ± sohbet oturumu yÃ¶neticisi."""
    
    def __init__(self, llm: ChatGoogleGenerativeAI, checkpointer: MemorySaver):
        self.llm = llm
        self.checkpointer = checkpointer
        self.graph = self._build_graph()
        self.thread_id = "default"  # Ã‡ok kullanÄ±cÄ±lÄ± senaryolar iÃ§in deÄŸiÅŸtirilebilir
    
    def _build_graph(self) -> StateGraph:
        """LangGraph iÅŸ akÄ±ÅŸÄ±nÄ± oluÅŸtur."""
        # Graph oluÅŸtur
        workflow = StateGraph(ChatState)
        
        # Node'larÄ± ekle
        workflow.add_node("router", lambda state: router_node(state, self.llm))
        workflow.add_node("retrieve", retrieve_node)
        workflow.add_node("llm", lambda state: llm_node(state, self.llm))
        
        # Edge'leri ekle
        workflow.add_edge(START, "router")
        
        # Conditional edge: router'dan sonra retrieval gerekli mi?
        workflow.add_conditional_edges(
            "router",
            decide_next_node,  # Karar fonksiyonu
            {
                "retrieve": "retrieve",  # PENDING ise retrieval'a git
                "llm": "llm"  # NOT_NEEDED ise direkt LLM'e git
            }
        )
        
        workflow.add_edge("retrieve", "llm")  # Retrieve sonrasÄ± LLM'e git
        workflow.add_edge("llm", END)
        
        # Checkpointer ile derle
        return workflow.compile(checkpointer=self.checkpointer)
    
    def get_config(self):
        """thread_id ile konfigÃ¼rasyon al."""
        return {"configurable": {"thread_id": self.thread_id}}
    
    def get_state(self) -> dict:
        """Mevcut state'i dict olarak al."""
        state = self.graph.get_state(self.get_config())
        if state and state.values:
            # TÃ¼m key'lerin varsayÄ±lanlarÄ±nÄ± garanti et
            values = state.values
            return {
                "levels": values.get("levels"),
                "messages": values.get("messages", []),
                "retrieved_docs": values.get("retrieved_docs", [])
            }
        return {"levels": None, "messages": [], "retrieved_docs": []}

    def draw_graph_mermaid(self) -> str:
        """Graph yapÄ±sÄ±nÄ± Mermaid diyagram olarak dÃ¶ndÃ¼r."""
        return self.graph.get_graph().draw_mermaid()

    def draw_graph_png(self, output_path: str) -> None:
        """Graph'Ä± verilen yola PNG dosyasÄ± olarak Ã§iz."""
        self.graph.get_graph().draw_png(output_path)
    
    def clear_history(self):
        """Thread'i sÄ±fÄ±rlayarak sohbet geÃ§miÅŸini temizle."""
        self.thread_id = f"thread_{os.urandom(8).hex()}"
    
    def set_levels(self, levels: list[str]):
        """State'te eÄŸitim kademelerini ayarla."""
        # BasitÃ§e state'i yeni kademelerle gÃ¼ncelle
        self.graph.update_state(self.get_config(), {"levels": levels})
    
    def chat(self, user_query: str) -> str:
        """KullanÄ±cÄ± mesajÄ±nÄ± gÃ¶nder ve yanÄ±t al."""
        try:
            # Ä°ÅŸlemeden Ã¶nce kademe bahsini tespit et
            level_detection = detect_level_mentions(self.llm, user_query)
            
            # Gerekirse tespit edilen kademeleri baÄŸlama ekle
            if level_detection.should_add_to_context and level_detection.detected_levels:
                current_state = self.get_state()
                current_levels = list(current_state.get("levels") or [])
                
                levels_added = []
                for level in level_detection.detected_levels:
                    level = level.lower()
                    if level in SUPPORTED_LEVELS and level not in current_levels:
                        current_levels.append(level)
                        levels_added.append(level)
                
                # State'i yeni kademelerle gÃ¼ncelle
                if levels_added:
                    self.graph.update_state(self.get_config(), {"levels": current_levels})
            
            # MesajÄ± graph Ã¼zerinden iÅŸle
            result = self.graph.invoke(
                {"messages": [HumanMessage(content=user_query)]},
                self.get_config()
            )
            
            # SonuÃ§tan son AI mesajÄ±nÄ± Ã§Ä±kar
            if result and "messages" in result:
                for msg in reversed(result["messages"]):
                    if isinstance(msg, AIMessage):
                        return msg.content
            
            return "ÃœzgÃ¼nÃ¼m, bir yanÄ±t Ã¼retemedim. LÃ¼tfen sorunuzu farklÄ± ÅŸekilde sormayÄ± deneyin."
        
        except Exception as e:
            traceback.print_exc()
            return "ÃœzgÃ¼nÃ¼m, teknik bir sorun oluÅŸtu. LÃ¼tfen tekrar deneyin."

# CLI iÃ§in main() fonksiyonu kaldÄ±rÄ±ldÄ±
# Production'da Streamlit (app.py) kullanÄ±lÄ±yor

# --- GRAPH VISUALIZATION ---
if __name__ == "__main__":
    """Graph yapÄ±sÄ±nÄ± gÃ¶rselleÅŸtir - development amaÃ§lÄ±"""
    print("ğŸ¨ LangGraph yapÄ±sÄ± oluÅŸturuluyor...")
    
    # LLM ve checkpointer oluÅŸtur
    llm = initialize_chat_model()
    checkpointer = MemorySaver()
    
    # ChatSession oluÅŸtur
    session = ChatSession(llm, checkpointer)
    
    # PNG olarak kaydet
    output_path = "langgraph_visualization.png"
    try:
        session.draw_graph_png(output_path)
        print(f"âœ… Graph baÅŸarÄ±yla kaydedildi: {output_path}")
    except Exception as e:
        print(f"âŒ PNG oluÅŸturulamadÄ±: {e}")
        print("ğŸ’¡ Not: 'pygraphviz' veya 'graphviz' kurulu olmalÄ±")
        print("   macOS: brew install graphviz && pip install pygraphviz")
    
    # Mermaid diyagramÄ± da yazdÄ±r
    print("\nğŸ“Š Mermaid Diyagram:")
    print("=" * 70)
    print(session.draw_graph_mermaid())
    print("=" * 70)
